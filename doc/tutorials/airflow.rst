Airflow
=======

This tutorial shows you how to export a Ploomber pipeline using Soopervisor.

If you encounter any issues with this
tutorial, `let us know <https://github.com/ploomber/soopervisor/issues/new?title=Airflow%20tutorial%20problem>`_.


Pre-requisites
**************
* ``airflow`` `See instructions here <https://airflow.apache.org/docs/apache-airflow/stable/start/index.html>`_.
* ``conda`` `See instruction shere <https://docs.conda.io/en/latest/miniconda.html>`_
* `git <https://git-scm.com/book/en/v2/Getting-Started-Installing-Git>`_
* Install Ploomber with ``pip install ploomber``


Instructions
------------

Once you installed and configured Airflow, start the scheduler and
webserver. In a terminal:

NOTE: soopervisor and ploomber must be installed in the host

.. code-block:: sh

    airflow webserver --port 8080

.. note::

    To log in to the web server, you must the credentials configured as part
    of the setup process by running the ``airflow users create`` command.

In a second terminal:

.. code-block:: sh

    airflow scheduler


Let's now pull some sample code:

.. code-block:: sh

    # get the sample projects
    git clone https://github.com/ploomber/projects
    cd projects/ml-intermediate/

    # configure environment
    conda env create --file environment.yml

    # activate environment
    conda activate ml-intermediate

    # generate lock file
    conda env export --no-build --file environment.lock.yml


We now export the pipeline to Airflow:

.. code-block:: sh

    soopervisor add train --backend airflow


.. note::

    You don't have to install ``soopervisor`` manually; it should've been
    installed when running ``ploomber install``. If missing, install it with
    ``pip install soopervisor``.


TODO: explain env.train.yaml addition

.. code-block:: yaml
    
    product_root: /some/output/directory


.. code-block:: sh

    soopervisor submit train


Once the export process finishes, you'll see a new ``train/`` folder with
two subfolders ``dag/``, which contains the Airflow DAG definition and
``ploomber/`` which contains your project's source code. To deploy, move
those directories to your ``AIRFLOW_HOME``.

For example, if ``AIRFLOW_HOME`` is set to ``~/airflow``:


.. code-block:: sh

    cp train/dags/ml-intermediate.py ~/airflow/dags/ml-intermediate.py
    cp -r train/ploomber/ml-intermediate  ~/airflow/ploomber


airflow dags list
airflow dags unpause ml-intermediate
airflow dags trigger ml-intermediate
airflow dags state ml-intermediate "2021-05-19 20:55:42+00:00"

Generated Airflow DAG
---------------------

The generated Airflow pipeline consists of ``BashOperator`` tasks, one
per task in the original Ploomber pipeline. Each task runs a script that
creates a conda virtual environment and runs the task.

The generated file is simple, and you can customize it to your needs by
using Airflow's API directly.

.. note::
    
    We are adding more features such as using other types of
    operators for exported tasks. Let us know what we should build next
    by opening an issue in the `repository <https://github.com/ploomber/soopervisor>`_.


Requirements in the Airflow host
--------------------------------

For Airflow to parse the DAG, it must have ``soopervisor`` installed. To
execute the pipeline; it must have ``conda`` installed.

Examples
--------

The sample projects repository contains a few example pipelines that can be
exported to Airflow:

Before running the examples, make sure you have an Airflow installation
available. `Check out Airflow's documentation for instructions <https://airflow.apache.org/docs/apache-airflow/stable/start/index.html>`_.

.. code-block:: sh

    git clone https://github.com/ploomber/projects
    cd projects/


    # export a few projects
    cd ml-intermediate
    soopervisor add train --backend airflow

    cd ../etl
    soopervisor add train --backend airflow


Storing pipeline artifacts
--------------------------

It's common to store the artifacts generated by your pipeline
(data files, trained models, etc.) for later review. We currently support
uploading to Google Cloud Storage and Amazon S3
`click here <https://github.com/ploomber/projects/blob/master/ml-basic/pipeline.yaml>`_ to see an example.

If all tasks execute in the same machine, configuring remote storage helps you
store any generated artifacts. However, if you're using a distributed
executor (e.g., celery), storing your pipeline artifacts guarantees
that downstream tasks have access to their inputs (which are the outputs
from upstream tasks).


Optional: Parametrizing your pipeline
-------------------------------------

Say you are developing a pipeline, you might choose a folder to save all
outputs (such as ``/data/project/output``. When you deploy to Airflow, it is
unlikely that you have the same filesystem; hence, you would choose a different
folder (say ``/airflow-data/project/output``).

Ploomber provides a clean way of achieving this
using `parametrization <https://ploomber.readthedocs.io/en/stable/user-guide/parametrized.html>`_, the basic idea is that you can parametrize where your pipeline saves its output.

To achieve this, Soopervisor looks for an ``env.{name}.yaml`` file used to
load your Airflow pipeline. This way, you can keep development and production
configurations separated. Replace env with the target name, which is the first
argument passed to ``soopervisor add``, in our case: ``train``.

One important thing to keep in mind is that when using Airflow, you should not
store pipeline product's inside the project's root folder because this can
negatively impact Airflow's performance, which continuously scans folders
looking for new pipeline definitions. To prevent this from happening,
Soopervisor analyzes your pipeline during the export process and shows you
an error message if any pipeline task will attempt to save files inside
the project's root folder.
