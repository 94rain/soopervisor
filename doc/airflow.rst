Airfow
======

*This tutorial shows you how to export a Ploomber pipeline using Soopervisor.*


Exporting a Ploomber pipeline using Soopervisor
-----------------------------------------------

Generating an Airflow pipeline from a Ploomber one is as simple as installing
Soopervisor and running one command:

.. code-block:: sh

    pip install soopervisor
    soopervisor add train --backend airflow


Once the export process finishes, you'll see a new ``train/`` folder with
two subfolders ``dag/``, which contains the Airflow DAG definition and
``ploomber/`` which contains your project's source code. To deploy, move
those directories to your AIRFLOW_HOME.

Generated Airflow DAG
---------------------

The generated Airflow pipeline consists of ``BashOperator`` tasks, one
per task in the original Ploomber pipeline. Each task runs a script that
creates a conda virtual environment and runs the task.

The generated file is simple, and you can customize it to your needs by
using Airflow's API directly.

*NOTE: we are adding more features such as using other types of
operators for exported tasks. Let us know what features we are missing
by opening an issue in the* `repository <https://github.com/ploomber/soopervisor>`_.


Requirements in the Airflow host
--------------------------------

For Airflow to parse the DAG, it must have ``soopervisor`` installed. To
execute the pipeline; it must have ``conda`` installed.

Examples
--------

The sample projects repository contains a few example pipelines that can be
exported to Airflow:

Before running the examples, make sure you have an Airflow installation
available. `Check out Airflow's documentation for instructions <https://airflow.apache.org/docs/apache-airflow/stable/start/index.html>`_.

.. code-block:: sh

    git clone https://github.com/ploomber/projects
    cd projects/


    # export a few projects
    cd ml-intermediate
    soopervisor add train --backend airflow

    cd ../etl
    soopervisor add train --backend airflow


Storing pipeline artifacts
--------------------------

It's common to store the artifacts generated by your pipeline
(data files, trained models, etc.) for later review. We currently support
uploading to Google Cloud Storage and Amazon S3
`click here <https://github.com/ploomber/projects/blob/master/ml-basic/pipeline.yaml>`_ to see an example.

If all tasks execute in the same machine, configuring remote storage helps you
store any generated artifacts. However, if you're using a distributed
executor (e.g., celery), storing your pipeline artifacts guarantees
that downstream tasks have access to their inputs (which are the outputs
from upstream tasks).


Optional: Parametrizing your pipeline
-------------------------------------

Say you are developing a pipeline, you might choose a folder to save all
outputs (such as ``/data/project/output``. When you deploy to Airflow, it is
unlikely that you have the same filesystem; hence, you would choose a different
folder (say ``/airflow-data/project/output``).

Ploomber provides a clean way of achieving this
using `parametrization <https://ploomber.readthedocs.io/en/stable/user-guide/parametrized.html>`_, the basic idea is that you can parametrize where your pipeline saves its output.

To achieve this, Soopervisor looks for an ``env.{name}.yaml`` file used to
load your Airflow pipeline. This way, you can keep development and production
configurations separated. Replace env with the target name, which is the first
argument passed to ``soopervisor add``, in our case: ``train``.

One important thing to keep in mind is that when using Airflow, you should not
store pipeline product's inside the project's root folder because this can
negatively impact Airflow's performance, which continuously scans folders
looking for new pipeline definitions. To prevent this from happening,
Soopervisor analyzes your pipeline during the export process and shows you
an error message if any pipeline task will attempt to save files inside
the project's root folder.
