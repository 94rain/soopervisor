Running in Kubernetes
=====================

Soopervisor can export Ploomber projects to run in Kubernetes via
`Argo <https://argoproj.github.io/argo/>`_.

Argo
----

Argo is a general-purpose framework tool to execute workflows in Kubernetes.
Argo workflows are written in YAML and it requires you to specify each task
in your pipeline, task dependencies, script to run, containers, mounted
volumes, etc. Soopervisor automates the creation of this YAML spec, to allow
your project to run in a Kubernetes cluster just as it runs in your computer.


Workflow
--------

Once you have your project ready, generate the Argo spec using:

.. code-block:: sh

    soopervisor export


To execute it:

.. code-block:: sh

    argo submit -n argo argo.yaml


Technical details
-----------------

This section describes details to interface with Argo and Kubernetes.

Argo only provides a way to submit workflows using YAML files. This implies
that you can customize the deployment process as you want. To execute a
Ploomber project in Kubernetes you have to ensure that 1) project's source
code is available on each Pod and 2) Pods can get their input data (which
is generated by their dependencies).

Soopervisor implements a simple deployment workflow but you can customize it
to suit your needs. Implementation details and possible customizations are
described in the following sections.

Generated Argo spec
*******************

``soopervisor export`` analyzes your pipeline and automatically generates the
Argo YAML spec. This involves generating one entry in the spec per pipeline
task and setting the same graph structure by indicating the dependencies for
each task.

Each Pod runs a single task using the ``continuumio/miniconda3`` image. The
script executed on each Pod sets up the conda environment using the
``environment.yml`` file, then executes the given task.


Currently, the spec expects a ``nfs`` `persistent volume clain (PVC) <https://kubernetes.io/docs/concepts/storage/persistent-volumes/>`_
to exist in the cluster and mounts a the folder ``/{project-name}`` to
``/mnt/vol`` to each Pod. Where ``{project-name}}`` is the name of your project
(the name of the folder that contains your ``pipeline.yaml`` file).


Project's source code
*********************

A Ploomber project is composed of a conda ``environment.yml``,
``pipeline.yaml`` and source code files (``.py``, ``.sql``, ``.R``, etc). The
simplest way to make the source code available to every Pod is to upload your
code to a persistent volume and mount it on every Pod when it starts execution.

The primary disadvantage is that there is no control over pipeline versions,
another way to solves this problem is to generate a package from your project
(each time with a different version number), upload it to a package registry
and have the pods pull the project from the registry.


Input data
**********

During pipeline execution, tasks get their inputs from upstream dependencies.
When running a pipeline in a single machine, this isn't a problem because
all files are saved to the same filesystem. When running in Kubernetes, each
Pod has its own filesystem. The simplest solution is to mount a shared disk
and have all tasks write their outputs to the shared resource. This reduces
the need to move large datasets over the network. Although simple, this
approach is sometimes unfeasible if the cluster spans several cloud regions
and it isn't possible to mount a shared disk on all pods.

An alternative approach is to have each task fetch its inputs over the network
before execution. Currently, there isn't any functionality to support this, if
you want to implement this logic, you have to implement it in your pipeline.


Full example
************

This section is a complete example to run a Ploomber project in Kubernetes
using Google Cloud. It assumes ``gcloud`` and ``kubectl`` are configured.

**Part 1: create a Kubernetes cluster and install Argo**

.. code-block:: sh

    # create cluster
    gcloud container clusters create my-cluster --num-nodes=1 --zone us-east1-b

    # install argo
    kubectl create ns argo
    kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml


Submit a sample workflow to make sure it's working:

.. code-block:: sh

    argo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml

**Part 2: Add a shared disk (NFS)**

.. code-block:: sh

    # create disk. make sure the zone matches your cluster
    gcloud compute disks create --size=10GB --zone=us-east1-b gce-nfs-disk

    # configure the nfs server
    kubectl apply -f 01-nfs-server.yaml

    # create service
    kubectl apply -f 02-nfs-service.yaml

    # check service
    kubectl get svc nfs-server

    # create persistent volume claim
    kubectl apply -f 03-nfs-pv-pvc.yaml

    # run sample workflow (uses nfs and creates an empty file on it)
    argo submit -n argo --watch dag.yaml

Container see the contents of the shared drive ``/export/`` directory at
``/mnt/vol``.

Check the output of ``dag.yaml``:

.. code-block:: sh

    # get nfs-server pod name
    kubectl get pod

    # replace with the name of the pod
    kubectl exec --stdin --tty {nfs-server-pod-name} -- /bin/bash

Once inside the Pod, run:

.. code-block:: sh

    ls /exports/

You should see files A, B, C, D. Generate by ``dag.yaml``.


**Part 3: Execute Ploomber sample projects**


.. code-block:: sh

    # port forwarding to enable the UI
    kubectl -n argo port-forward svc/argo-server 2746:2746


Then open: http://127.0.0.1:2746


.. code-block:: sh
    # get the sample projects
    git clone https://github.com/ploomber/projects

    # upload source code to the nfs server
    # (recommended: ml-basic/ (machine learning pipeline) and etl/)
    kubectl cp projects/ml-basic {nfs-server-pod-name}:/exports

    # generate argo spec
    soopervisor export

    # submit workflow
    argo submit -n argo --watch argo.yaml


**Make sure you delete your cluster after running this example.**