Running in Kubernetes
=====================

**Note:** This feature is in beta, if you are interested in being a beta tester, please open an issue in the repository

Soopervisor can export Ploomber projects to run in Kubernetes via
`Argo <https://argoproj.github.io/argo/>`_.

Argo
----

Argo is a general-purpose framework to execute, schedule and monitor workflows
in Kubernetes. Argo workflows are written in YAML and it requires you to
specify each task in your pipeline, task dependencies, script to run, Docker image to use,
mounted volumes, etc. This implies a steep (and unnecessary) learning curve
for a lot of people who can benefit from a production tool like Argo.

Soopervisor automates the creation of Argo's YAML spec. So users think in terms
of functions, scripts and notebooks, not in terms of clusters nor containers.

Soopervisor exports `Ploomber <https://github.com/ploomber/ploomber>`_ projects.
A Ploomber workflow can be specified via a YAML spec (there is also a Python
API for advanced use cases), which only requires users to tell what to run
(function/script/notebook) and where to save the output:

.. code-block:: yaml

    # Ploomber's "pipeline.yaml" example

    tasks:
    # tasks.get, features and join are python functions defined in tasks.py
    - source: tasks.get
      product: output/get.parquet

    - source: tasks.features
      product: output/features.parquet

    - source: tasks.join
      product: output/join.parquet

    # fit.ipynb is a notebook
    - source: fit.ipynb
      product:
        # where to save the executed copy
        nb: output/nb.ipynb
        # and any other generated files
        model: output/model.pickle


Execution order is inferred by building a directed acyclic graph through static
analysis in the source code.

`Click here <https://github.com/ploomber/projects/tree/master/ml-basic>`_ to
see the full code example.

Deployment process
------------------

Once you have a Ploomber project ready, generate the Argo spec using:

.. code-block:: sh

    soopervisor export


This command runs a few checks to make sure your pipeline is good to go,
and then generates the Argo's YAML spec. Once you upload your source code
to the cluster (strategies vary here). You can execute the workflow with:


.. code-block:: sh

    argo submit -n argo argo.yaml


By standardizing the deployment process (how to upload the code, which image
to use, how to mount volumes and how to install dependencies), end-users are
able to leverage Argo without having to deal with such nuances.

Technical details
-----------------

*This section describes the deailed process of interfacing Ploomber projects
with Argo/Kubernetes with a complete example*

``argo submit`` triggers a workflow by just uploading a YAML file, but it does
not take care of uploading anything else such as the project's source code.
This implies that to execute a  Ploomber project you have to ensure that
1) project's source code is available on each Pod and 2) Pods can get their
input data (which is generated by previous tasks).

Soopervisor implements a simple deployment workflow but you can customize it
to suit your needs. Implementation details and possible customizations are
described in the following sections.

Generated Argo spec
*******************

``soopervisor export`` analyzes your pipeline and automatically generates the
Argo YAML spec. This involves generating one entry in the spec per pipeline
task and setting the same graph structure by indicating the dependencies for
each task.

Each Pod runs a single task using the ``continuumio/miniconda3`` image. The
script executed on each Pod sets up the conda environment using the
user-provided ``environment.yml`` file, then executes the given task.


Currently, the spec expects a ``nfs`` `persistent volume clain (PVC) <https://kubernetes.io/docs/concepts/storage/persistent-volumes/>`_
to exist in the cluster and mounts the folder ``/{project-name}`` in such volume
to ``/mnt/vol`` on each Pod. Where ``{project-name}`` is the name of your project
(the name of the folder that contains your ``pipeline.yaml`` file).


Uploading project's source code
*******************************

A Ploomber project is composed of a conda ``environment.yml``,
``pipeline.yaml`` and source code files (``.py``, ``.sql``, ``.R``, etc). The
simplest way to make the source code available to every Pod is to upload your
code to a persistent volume and mount it on every Pod when it starts execution.

The primary disadvantage is that there is no control over pipeline versions,
another way to solve this is to generate a package from your project
(each time with a different version number), upload it to a package registry
and have the pods pull the project from the registry. An alternative approach
would be to fetch the source code from a repository. For simplicity, this
prototype directly uploads the source code from the client to a cluster shared
disk.

Input data
**********

During pipeline execution, tasks get their inputs from previous tasks (also
known as upstream dependencies). When running a pipeline in a single machine,
this works fine because all files are saved to the same filesystem. When
running in Kubernetes, each Pod has its own filesystem.

The simplest solution is to mount a shared disk and have all tasks write their
outputs to the shared resource. This reduces the need to move large datasets
over the network. Although simple, this approach is unfeasible if the cluster
spans several cloud regions and it isn't possible to mount a shared disk on all
pods.

An alternative approach is to have each task fetch its inputs over the network
before execution. The current prototype assumes all tasks write to a shared
disk.


Full example
************

This section is a complete example to run a Ploomber project in Kubernetes
using Google Cloud. ``gcloud`` and ``kubectl`` are configured.

**Part 1: create a Kubernetes cluster and install Argo**

.. code-block:: sh

    # create cluster
    gcloud container clusters create my-cluster --num-nodes=1 --zone us-east1-b

    # install argo
    kubectl create ns argo
    kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml


Submit a sample workflow to make sure Argo works:

.. code-block:: sh

    argo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml

**Part 2: Add a shared disk (NFS)**

.. code-block:: sh

    # create disk. make sure the zone matches your cluster
    gcloud compute disks create --size=10GB --zone=us-east1-b gce-nfs-disk

    # configure the nfs server
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/01-nfs-server.yaml
    kubectl apply -f 01-nfs-server.yaml

    # create service
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/02-nfs-service.yaml
    kubectl apply -f 02-nfs-service.yaml

    # check service
    kubectl get svc nfs-server

    # create persistent volume claim
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/03-nfs-pv-pvc.yaml
    kubectl apply -f 03-nfs-pv-pvc.yaml

    # run sample workflow (uses nfs and creates an empty file on it)
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/dag.yaml
    argo submit -n argo --watch dag.yaml

Container see the contents of the shared drive ``/export/`` directory at
``/mnt/vol``.

Check the output of ``dag.yaml``:

.. code-block:: sh

    # get nfs-server pod name
    kubectl get pod

    # replace with the name of the pod
    kubectl exec --stdin --tty {nfs-server-pod-name} -- /bin/bash

Once inside the Pod, run:

.. code-block:: sh

    ls /exports/

You should see files A, B, C, D. Generate by ``dag.yaml``.


**Part 3: Execute Ploomber sample projects**

Enable Argo's UI:

.. code-block:: sh

    # port forwarding to enable the UI
    kubectl -n argo port-forward svc/argo-server 2746:2746


Then open: http://127.0.0.1:2746


Run a Ploomber sample pipeline:

.. code-block:: sh

    # get the sample projects
    git clone https://github.com/ploomber/projects

    # get nfs pod name
    kubectl get pods -l role=nfs-server

    # upload source code to the nfs server
    # (recommended: ml-basic/ (machine learning pipeline) and etl/)
    kubectl cp projects/ml-basic {nfs-server-pod-name}:/exports

    # generate argo spec
    soopervisor export

    # submit workflow
    argo submit -n argo --watch argo.yaml


**Make sure you delete your cluster after running this example.**

